{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efe8aaf5-6322-4a74-b5c3-b68aa4caebbd",
   "metadata": {
    "execution": {
     "iopub.status.idle": "2025-07-29T20:07:47.958215Z",
     "shell.execute_reply": "2025-07-29T20:07:47.957644Z",
     "shell.execute_reply.started": "2025-07-29T20:07:04.379001Z"
    },
    "executionRoleArn": "arn:aws:iam::554739427960:role/service-role/AmazonEMRStudio_RuntimeRole_1753034729634",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22674d55dbcd4d3883757c3e0f0481e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr><th>ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>4</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"https://spark-live-ui.emr-serverless.amazonaws.com\" class=\"emr-proxy-link\" emr-runtime=\"emr-serverless\" emr-resource=\"00fudmv851ht791e\" application-id=\"00fu6dg36gjp2k1d\">Link</a></td><td></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType, TimestampType, LongType, StringType\n",
    "import pyspark.sql.functions as F\n",
    "import boto3\n",
    "from functools import reduce\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "s3_bucket_name = \"robot-dreams-source-data\"\n",
    "yellow_taxi_path = \"home-work-1/nyc_taxi/yellow\"\n",
    "green_taxi_path = \"home-work-1/nyc_taxi/green\"\n",
    "zone_path = \"home-work-1/nyc_taxi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19d08eae-8fe9-4db7-a18b-99d8829f41be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T20:07:47.959192Z",
     "iopub.status.busy": "2025-07-29T20:07:47.959051Z",
     "iopub.status.idle": "2025-07-29T20:07:49.199582Z",
     "shell.execute_reply": "2025-07-29T20:07:49.199167Z",
     "shell.execute_reply.started": "2025-07-29T20:07:47.959177Z"
    },
    "executionRoleArn": "arn:aws:iam::554739427960:role/service-role/AmazonEMRStudio_RuntimeRole_1753034729634",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "905c27a974d74026bb6b3be1bcde9dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def list_s3_parquet_files(bucket: str, prefix: str) -> list:\n",
    "    \"\"\"\n",
    "    Get list of parquet files from S3 by prefix.\n",
    "    \"\"\"\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    pages = paginator.paginate(Bucket=bucket, Prefix=prefix)\n",
    "\n",
    "    files = []\n",
    "    for page in pages:\n",
    "        for obj in page.get('Contents', []):\n",
    "            key = obj['Key']\n",
    "            if key.endswith('.parquet'):\n",
    "                files.append(f\"s3://{bucket}/{key}\")\n",
    "    return files\n",
    "\n",
    "def load_and_cast_parquet_files_s3(spark, bucket: str, prefix: str, schema_map: dict, source_label: str, rename_map: dict = None):\n",
    "    \"\"\"\n",
    "    Downloads parquet files from S3, converts them to a single schema, and merges them.\n",
    "\n",
    "    :param spark: SparkSession\n",
    "    :param bucket: S3 bucket name\n",
    "    :param prefix: prefix inside the bucket\n",
    "    :param schema_map: dictionary with types\n",
    "    :param source_label: string that will be added to the source column\n",
    "    :param rename_map: optional {old_col: new_col} to align different names\n",
    "    :return: merged DataFrame\n",
    "    \"\"\"\n",
    "    parquet_files = list_s3_parquet_files(bucket, prefix)\n",
    "    if not parquet_files:\n",
    "        raise Exception(f\"No parquet files found on path s3://{bucket}/{prefix}\")\n",
    "\n",
    "    dfs = []\n",
    "    for file_path in parquet_files:\n",
    "        df = spark.read.parquet(file_path)\n",
    "        \n",
    "        if rename_map:\n",
    "            for old_name, new_name in rename_map.items():\n",
    "                if old_name in df.columns:\n",
    "                    df = df.withColumnRenamed(old_name, new_name)\n",
    "\n",
    "        for col_name, dtype in schema_map.items():\n",
    "            if col_name in df.columns:\n",
    "                df = df.withColumn(col_name, F.col(col_name).cast(dtype))\n",
    "        \n",
    "        # add taxi_type column\n",
    "        df = df.withColumn(\"taxi_type\", F.lit(source_label))\n",
    "        dfs.append(df)\n",
    "\n",
    "        # Union all dataframes\n",
    "        unified_df = reduce(lambda x, y: x.unionByName(y, allowMissingColumns=True), dfs)\n",
    "\n",
    "    return unified_df\n",
    "\n",
    "\n",
    "def save_single_parquet_to_s3(df, temp_path, bucket, tmp_prefix, final_prefix, spark, wait_seconds=5):\n",
    "    \"\"\"\n",
    "    Saves a Spark DataFrame as a single Parquet file to S3 and renames the part file.\n",
    "\n",
    "    :param df: Spark DataFrame\n",
    "    :param temp_path:\n",
    "    :param bucket: s3 bucket name\n",
    "    :param tmp_prefix: temporary path to save part-file\n",
    "    :param final_prefix: path to save renamed file\n",
    "    :param spark: SparkSession\n",
    "    :param wait_seconds: seconds to wait for consistency (default: 5)\n",
    "    \"\"\"\n",
    "    # Save DataFrame to temporary path in S3 (as part-*.parquet)\n",
    "    df.coalesce(1).write.mode(\"overwrite\").parquet(temp_path)\n",
    "\n",
    "    # Use boto3 to find the part file\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    \n",
    "    time.sleep(wait_seconds)  # Wait for eventual consistency in S3\n",
    "\n",
    "    response = s3.list_objects_v2(Bucket=bucket, Prefix=tmp_prefix)\n",
    "    part_key = None\n",
    "    for obj in response.get(\"Contents\", []):\n",
    "        if obj[\"Key\"].endswith(\".parquet\") and \"part-\" in obj[\"Key\"]:\n",
    "            part_key = obj[\"Key\"]\n",
    "            break\n",
    "\n",
    "    if not part_key:\n",
    "        raise Exception(f\"No part file found in {temp_path}\")\n",
    "\n",
    "    # Copy and rename the part file to final path\n",
    "    s3.copy_object(\n",
    "        Bucket=bucket,\n",
    "        CopySource={\"Bucket\": bucket, \"Key\": part_key},\n",
    "        Key=final_prefix\n",
    "    )\n",
    "\n",
    "    # Delete original part file\n",
    "    s3.delete_object(Bucket=bucket, Key=part_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e5c4306-2be4-475f-9814-6c15c95f6d7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T20:07:49.200475Z",
     "iopub.status.busy": "2025-07-29T20:07:49.200286Z",
     "iopub.status.idle": "2025-07-29T20:11:17.215168Z",
     "shell.execute_reply": "2025-07-29T20:11:17.214640Z",
     "shell.execute_reply.started": "2025-07-29T20:07:49.200459Z"
    },
    "executionRoleArn": "arn:aws:iam::554739427960:role/service-role/AmazonEMRStudio_RuntimeRole_1753034729634",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bda3922162de4c5fb0d417db81a27eb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/boto3/compat.py:82: PythonDeprecationWarning: Boto3 will no longer support Python 3.7 starting December 13, 2023. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.8 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)"
     ]
    }
   ],
   "source": [
    "schema = {\n",
    "    \"VendorID\": LongType(),\n",
    "    \"pickup_datetime\": StringType(),\n",
    "    \"dropoff_datetime\": StringType(),\n",
    "    \"passenger_count\": LongType(),\n",
    "    \"trip_distance\": DoubleType(),\n",
    "    \"RatecodeID\": LongType(),\n",
    "    \"store_and_fwd_flag\": StringType(),\n",
    "    \"PULocationID\": LongType(),\n",
    "    \"DOLocationID\": LongType(),\n",
    "    \"payment_type\": LongType(),\n",
    "    \"fare_amount\": DoubleType(),\n",
    "    \"extra\": DoubleType(),\n",
    "    \"mta_tax\": DoubleType(),\n",
    "    \"tip_amount\": DoubleType(),\n",
    "    \"tolls_amount\": DoubleType(),\n",
    "    \"ehail_fee\": DoubleType(),\n",
    "    \"improvement_surcharge\": DoubleType(),\n",
    "    \"total_amount\": DoubleType(),\n",
    "    \"trip_type\": DoubleType(),\n",
    "    \"congestion_surcharge\": DoubleType(),\n",
    "    \"airport_fee\": DoubleType()\n",
    "}\n",
    "rename_map = {\n",
    "    \"lpep_pickup_datetime\": \"pickup_datetime\",\n",
    "    \"tpep_pickup_datetime\": \"pickup_datetime\",\n",
    "    \"lpep_dropoff_datetime\": \"dropoff_datetime\",\n",
    "    \"tpep_dropoff_datetime\": \"dropoff_datetime\"\n",
    "}\n",
    "\n",
    "df_green = load_and_cast_parquet_files_s3(\n",
    "    spark,\n",
    "    bucket=s3_bucket_name,\n",
    "    prefix=green_taxi_path,\n",
    "    source_label = \"green\",\n",
    "    schema_map=schema,\n",
    "    rename_map=rename_map\n",
    ")\n",
    "\n",
    "df_yellow = load_and_cast_parquet_files_s3(\n",
    "    spark,\n",
    "    bucket=s3_bucket_name,\n",
    "    prefix=yellow_taxi_path,\n",
    "    source_label = \"yellow\",\n",
    "    schema_map=schema,\n",
    "    rename_map=rename_map\n",
    ")\n",
    "\n",
    "all_taxi_df = df_green.unionByName(df_yellow, allowMissingColumns=True)\n",
    "\n",
    "# Add columns pickup_hour, pickup_day_of_week, duration_min and filter df\n",
    "raw_trips_wo_zones_df = all_taxi_df \\\n",
    "    .withColumn(\"pickup_ts\", F.to_timestamp(\"pickup_datetime\")) \\\n",
    "    .withColumn(\"dropoff_ts\", F.to_timestamp(\"dropoff_datetime\")) \\\n",
    "    .withColumn(\"pickup_hour\", F.hour(\"pickup_datetime\")) \\\n",
    "    .withColumn(\"pickup_day_of_week\", F.date_format(\"pickup_ts\", \"EEEE\")) \\\n",
    "    .withColumn(\"duration_min\",\n",
    "                F.round((F.unix_timestamp(\"dropoff_ts\") - F.unix_timestamp(\"pickup_ts\")) / 60, 2)\n",
    "    ) \\\n",
    "    .filter(\n",
    "        ( F.col(\"trip_distance\") >= 0.1) &\n",
    "        ( F.col(\"fare_amount\") >= 2) &\n",
    "        ( F.col(\"duration_min\") >= 1)\n",
    "    )\n",
    "\n",
    "zones_df = spark.read.csv(f\"s3://{s3_bucket_name}/{zone_path}/taxi_zone_lookup.csv\", header=True, inferSchema=True)\n",
    "raw_trips_df = raw_trips_wo_zones_df \\\n",
    "    .join(\n",
    "        zones_df.selectExpr(\"LocationID as PULocationID\", \"Zone as pickup_zone\"),\n",
    "        on=\"PULocationID\",\n",
    "        how=\"left\"\n",
    "    ) \\\n",
    "    .join(\n",
    "        zones_df.selectExpr(\"LocationID as DOLocationID\", \"Zone as dropoff_zone\"),\n",
    "        on=\"DOLocationID\",\n",
    "        how=\"left\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "656a6fe0-0f4e-42ff-be74-017661ce321a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T20:11:17.216177Z",
     "iopub.status.busy": "2025-07-29T20:11:17.215935Z",
     "iopub.status.idle": "2025-07-29T20:11:17.541464Z",
     "shell.execute_reply": "2025-07-29T20:11:17.541059Z",
     "shell.execute_reply.started": "2025-07-29T20:11:17.216161Z"
    },
    "executionRoleArn": "arn:aws:iam::554739427960:role/service-role/AmazonEMRStudio_RuntimeRole_1753034729634",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b41931795e3c466289b09c4035fe9e71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Aggregation of trips by pickup zones\n",
    "zone_summary = raw_trips_df.groupBy(\"pickup_zone\").agg(\n",
    "    F.count(\"*\").alias(\"total_trips\"),\n",
    "    F.round(F.avg(\"trip_distance\"),3).alias(\"avg_trip_distance\"),\n",
    "    F.round(F.avg(\"total_amount\"),2).alias(\"avg_total_amount\"),\n",
    "    F.round(F.avg(\"tip_amount\"),2).alias(\"avg_tip_amount\"),\n",
    "    F.sum(F.when(F.col(\"taxi_type\") == \"yellow\", 1).otherwise(0)).alias(\"yellow_trips\"),\n",
    "    F.sum(F.when(F.col(\"taxi_type\") == \"green\", 1).otherwise(0)).alias(\"green_trips\"),\n",
    "    F.max(\"trip_distance\").alias(\"max_trip_distance\"),\n",
    "    F.min(\"tip_amount\").alias(\"min_tip_amount\")\n",
    ").withColumn(\n",
    "    \"yellow_share\",\n",
    "    F.when(F.col(\"total_trips\") != 0, F.round(F.col(\"yellow_trips\") / F.col(\"total_trips\"),2)).otherwise(0)\n",
    ").withColumn(\n",
    "    \"green_share\",\n",
    "    F.when(F.col(\"total_trips\") != 0, F.round(F.col(\"green_trips\") / F.col(\"total_trips\"),2)).otherwise(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2f80d58-a922-4fe0-91dd-c0e97d6fc765",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T20:11:17.542335Z",
     "iopub.status.busy": "2025-07-29T20:11:17.542146Z",
     "iopub.status.idle": "2025-07-29T20:12:58.079062Z",
     "shell.execute_reply": "2025-07-29T20:12:58.078466Z",
     "shell.execute_reply.started": "2025-07-29T20:11:17.542319Z"
    },
    "executionRoleArn": "arn:aws:iam::554739427960:role/service-role/AmazonEMRStudio_RuntimeRole_1753034729634",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "512b1ba1b48f467085023a67d0c73ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save result zone statistic\n",
    "date = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "bucket_for_results = \"vsentishcheva-hw\"\n",
    "tmp_prefix = f\"results/tmp_folder/{date}\"\n",
    "tmp_path = f\"s3://{bucket_for_results}/{tmp_prefix}\"\n",
    "\n",
    "zone_file_name = \"zone_statistic.parquet\"\n",
    "zone_prefix = f\"results/zone_statistic/{date}/{zone_file_name}\"\n",
    "\n",
    "save_single_parquet_to_s3(\n",
    "    df=zone_summary,\n",
    "    temp_path=tmp_path,\n",
    "    bucket = bucket_for_results,\n",
    "    tmp_prefix=tmp_prefix,\n",
    "    final_prefix=zone_prefix,\n",
    "    spark=spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00cf9c26-2002-417a-8da7-b9456810cc51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T20:12:58.080023Z",
     "iopub.status.busy": "2025-07-29T20:12:58.079876Z",
     "iopub.status.idle": "2025-07-29T20:12:58.429083Z",
     "shell.execute_reply": "2025-07-29T20:12:58.428689Z",
     "shell.execute_reply.started": "2025-07-29T20:12:58.080006Z"
    },
    "executionRoleArn": "arn:aws:iam::554739427960:role/service-role/AmazonEMRStudio_RuntimeRole_1753034729634",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad51f02c1315495ea6bf78360b42752c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Aggregation of trips by pickup zones and days of the week\n",
    "zone_days_summary = raw_trips_df.groupBy(\"pickup_zone\", \"pickup_day_of_week\").agg(\n",
    "    F.count(\"*\").alias(\"total_trips\"),\n",
    "    F.round((F.sum(F.when(F.col(\"total_amount\") > 30, 1).otherwise(0)) / F.count(\"*\")),2).alias(\"high_fare_share\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f3e6c18-9ee0-4afa-b886-d8947adb15e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-29T20:12:58.429866Z",
     "iopub.status.busy": "2025-07-29T20:12:58.429697Z",
     "iopub.status.idle": "2025-07-29T20:13:43.011719Z",
     "shell.execute_reply": "2025-07-29T20:13:43.011242Z",
     "shell.execute_reply.started": "2025-07-29T20:12:58.429850Z"
    },
    "executionRoleArn": "arn:aws:iam::554739427960:role/service-role/AmazonEMRStudio_RuntimeRole_1753034729634",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae9b0f841bf844b290711dcf1f02af96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save result zone days statstic\n",
    "\n",
    "zone_days_file_name = \"zone_days_statistic.parquet\"\n",
    "zone_days_prefix = f\"results/zone_days_statistic/{date}/{zone_days_file_name}\"\n",
    "\n",
    "save_single_parquet_to_s3(\n",
    "    df=zone_summary,\n",
    "    temp_path=tmp_path,\n",
    "    bucket = bucket_for_results,\n",
    "    tmp_prefix=tmp_prefix,\n",
    "    final_prefix=zone_days_prefix,\n",
    "    spark=spark)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "spark_magic_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
